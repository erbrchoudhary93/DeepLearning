{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bac9b8",
   "metadata": {},
   "source": [
    "# Install required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0212fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] [-e] <vcs project url> ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] [-e] <local project path> ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Usage:   \n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] [-e] <vcs project url> ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] [-e] <local project path> ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Usage:   \n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] [-e] <vcs project url> ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] [-e] <local project path> ...\n",
      "  /home/ram/Desktop/DeepLearning/env/bin/python -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -u\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install required packages for the project\n",
    "\n",
    "%pip install torch torchvision torchaudio -quit --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -quit matplotlib\n",
    "%pip install -quit safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123fddd5",
   "metadata": {},
   "source": [
    "##   import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31702f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701a69d",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb8e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\n",
    "    root='.',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f25b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.MNIST(\n",
    "    root='.',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a782120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f52e73",
   "metadata": {},
   "source": [
    "## Converting data in to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c26f90ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 157\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "print(len(train_loader), len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e21c106",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Train the model\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "# Evaluate the model\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return total_loss / len(test_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab9b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self,in_dim,n_class):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.n_class = n_class\n",
    "        self.linear = nn.Linear(in_dim, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "776e53d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(28*28,10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "992185ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['linear.weight', 'linear.bias'])\n",
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "model.state_dict()\n",
    "print(model.state_dict().keys())\n",
    "print(model.state_dict()['linear.weight'].shape)\n",
    "print(model.state_dict()['linear.bias'].shape)\n",
    "# Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7bf64a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0214,  0.0049,  0.0297,  ...,  0.0075,  0.0281, -0.0100],\n",
      "        [ 0.0162, -0.0328, -0.0171,  ..., -0.0237,  0.0334,  0.0348],\n",
      "        [-0.0283, -0.0050,  0.0241,  ..., -0.0137, -0.0225,  0.0129],\n",
      "        ...,\n",
      "        [-0.0087, -0.0295,  0.0104,  ..., -0.0213,  0.0223,  0.0041],\n",
      "        [ 0.0108,  0.0318, -0.0196,  ...,  0.0088,  0.0020,  0.0152],\n",
      "        [ 0.0354,  0.0178,  0.0205,  ..., -0.0296, -0.0231,  0.0353]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0116,  0.0311, -0.0334, -0.0127, -0.0058,  0.0154, -0.0297,  0.0171,\n",
      "         0.0330,  0.0333], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "822d27ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# For CuDA support, check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70de8ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# for MPS support (Apple Silicon Macs)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b88cc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "learning_rate = 1e-2\n",
    "momentum = 0.9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b0b6ad",
   "metadata": {},
   "source": [
    "# Building Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8c3cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Epoch [1/5], Step [100/938], Loss: 0.2694, Accuracy: 0.7739\n",
      "Epoch [1/5], Step [200/938], Loss: 0.4029, Accuracy: 0.8227\n",
      "Epoch [1/5], Step [300/938], Loss: 0.5392, Accuracy: 0.8433\n",
      "Epoch [1/5], Step [400/938], Loss: 0.2799, Accuracy: 0.8550\n",
      "Epoch [1/5], Step [500/938], Loss: 0.2438, Accuracy: 0.8629\n",
      "Epoch [1/5], Step [600/938], Loss: 0.4277, Accuracy: 0.8684\n",
      "Epoch [1/5], Step [700/938], Loss: 0.3795, Accuracy: 0.8728\n",
      "Epoch [1/5], Step [800/938], Loss: 0.2202, Accuracy: 0.8763\n",
      "Epoch [1/5], Step [900/938], Loss: 0.2066, Accuracy: 0.8791\n",
      "Epoch [1/5] completed in 19.07 seconds, Loss: 0.4003, Accuracy: 0.8799\n",
      "********************\n",
      "Epoch [2/5], Step [100/938], Loss: 0.2379, Accuracy: 0.9059\n",
      "Epoch [2/5], Step [200/938], Loss: 0.3208, Accuracy: 0.9073\n",
      "Epoch [2/5], Step [300/938], Loss: 0.2042, Accuracy: 0.9045\n",
      "Epoch [2/5], Step [400/938], Loss: 0.2811, Accuracy: 0.9048\n",
      "Epoch [2/5], Step [500/938], Loss: 0.3660, Accuracy: 0.9048\n",
      "Epoch [2/5], Step [600/938], Loss: 0.3481, Accuracy: 0.9054\n",
      "Epoch [2/5], Step [700/938], Loss: 0.2649, Accuracy: 0.9053\n",
      "Epoch [2/5], Step [800/938], Loss: 0.4556, Accuracy: 0.9053\n",
      "Epoch [2/5], Step [900/938], Loss: 0.1785, Accuracy: 0.9052\n",
      "Epoch [2/5] completed in 21.72 seconds, Loss: 0.3265, Accuracy: 0.9052\n",
      "********************\n",
      "Epoch [3/5], Step [100/938], Loss: 0.3424, Accuracy: 0.9019\n",
      "Epoch [3/5], Step [200/938], Loss: 0.2849, Accuracy: 0.9063\n",
      "Epoch [3/5], Step [300/938], Loss: 0.2202, Accuracy: 0.9077\n",
      "Epoch [3/5], Step [400/938], Loss: 0.1850, Accuracy: 0.9088\n",
      "Epoch [3/5], Step [500/938], Loss: 0.1126, Accuracy: 0.9089\n",
      "Epoch [3/5], Step [600/938], Loss: 0.4252, Accuracy: 0.9098\n",
      "Epoch [3/5], Step [700/938], Loss: 0.3341, Accuracy: 0.9096\n",
      "Epoch [3/5], Step [800/938], Loss: 0.2487, Accuracy: 0.9105\n",
      "Epoch [3/5], Step [900/938], Loss: 0.2935, Accuracy: 0.9111\n",
      "Epoch [3/5] completed in 19.09 seconds, Loss: 0.3110, Accuracy: 0.9109\n",
      "********************\n",
      "Epoch [4/5], Step [100/938], Loss: 0.2038, Accuracy: 0.9158\n",
      "Epoch [4/5], Step [200/938], Loss: 0.3661, Accuracy: 0.9127\n",
      "Epoch [4/5], Step [300/938], Loss: 0.4888, Accuracy: 0.9132\n",
      "Epoch [4/5], Step [400/938], Loss: 0.4261, Accuracy: 0.9136\n",
      "Epoch [4/5], Step [500/938], Loss: 0.4238, Accuracy: 0.9117\n",
      "Epoch [4/5], Step [600/938], Loss: 0.3457, Accuracy: 0.9123\n",
      "Epoch [4/5], Step [700/938], Loss: 0.2284, Accuracy: 0.9129\n",
      "Epoch [4/5], Step [800/938], Loss: 0.4524, Accuracy: 0.9134\n",
      "Epoch [4/5], Step [900/938], Loss: 0.1690, Accuracy: 0.9134\n",
      "Epoch [4/5] completed in 19.20 seconds, Loss: 0.3056, Accuracy: 0.9126\n",
      "********************\n",
      "Epoch [5/5], Step [100/938], Loss: 0.4501, Accuracy: 0.9162\n",
      "Epoch [5/5], Step [200/938], Loss: 0.1668, Accuracy: 0.9162\n",
      "Epoch [5/5], Step [300/938], Loss: 0.4639, Accuracy: 0.9165\n",
      "Epoch [5/5], Step [400/938], Loss: 0.2118, Accuracy: 0.9173\n",
      "Epoch [5/5], Step [500/938], Loss: 0.1988, Accuracy: 0.9162\n",
      "Epoch [5/5], Step [600/938], Loss: 0.5200, Accuracy: 0.9156\n",
      "Epoch [5/5], Step [700/938], Loss: 0.3885, Accuracy: 0.9153\n",
      "Epoch [5/5], Step [800/938], Loss: 0.0960, Accuracy: 0.9158\n",
      "Epoch [5/5], Step [900/938], Loss: 0.4157, Accuracy: 0.9160\n",
      "Epoch [5/5] completed in 19.64 seconds, Loss: 0.2959, Accuracy: 0.9161\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"*\"*20)\n",
    "    start_time = time.time()\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    model.train()\n",
    "    for i ,data in enumerate(train_loader,1):\n",
    "        images, labels = data\n",
    "        # Flatten the images\n",
    "        images = images.view(images.size(0), -1)  # Flatten the images\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # running_acc += (predicted == labels).sum().item()\n",
    "        running_acc += (predicted == labels).float().mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {running_acc / i:.4f}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {end_time - start_time:.2f} seconds, Loss: {running_loss / len(train_loader):.4f}, Accuracy: {running_acc / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ccb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d100ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Epoch [1/5], Step [100/938], Loss: 0.2130, Accuracy: 0.9120\n",
      "Epoch [1/5], Step [200/938], Loss: 0.0995, Accuracy: 0.9145\n",
      "Epoch [1/5], Step [300/938], Loss: 0.2021, Accuracy: 0.9166\n",
      "Epoch [1/5], Step [400/938], Loss: 0.3490, Accuracy: 0.9173\n",
      "Epoch [1/5], Step [500/938], Loss: 0.3042, Accuracy: 0.9183\n",
      "Epoch [1/5], Step [600/938], Loss: 0.3458, Accuracy: 0.9187\n",
      "Epoch [1/5], Step [700/938], Loss: 0.2701, Accuracy: 0.9183\n",
      "Epoch [1/5], Step [800/938], Loss: 0.3535, Accuracy: 0.9185\n",
      "Epoch [1/5], Step [900/938], Loss: 0.1898, Accuracy: 0.9178\n",
      "55072 60000\n",
      "Epoch [1/5] completed in 22.44 seconds, Loss: 0.2949, Accuracy: 0.9179\n",
      "********************\n",
      "Epoch [2/5], Step [100/938], Loss: 0.2532, Accuracy: 0.9144\n",
      "Epoch [2/5], Step [200/938], Loss: 0.4482, Accuracy: 0.9191\n",
      "Epoch [2/5], Step [300/938], Loss: 0.2041, Accuracy: 0.9190\n",
      "Epoch [2/5], Step [400/938], Loss: 0.3768, Accuracy: 0.9184\n",
      "Epoch [2/5], Step [500/938], Loss: 0.2083, Accuracy: 0.9189\n",
      "Epoch [2/5], Step [600/938], Loss: 0.1566, Accuracy: 0.9176\n",
      "Epoch [2/5], Step [700/938], Loss: 0.0853, Accuracy: 0.9170\n",
      "Epoch [2/5], Step [800/938], Loss: 0.1638, Accuracy: 0.9175\n",
      "Epoch [2/5], Step [900/938], Loss: 0.2396, Accuracy: 0.9170\n",
      "55012 60000\n",
      "Epoch [2/5] completed in 18.62 seconds, Loss: 0.2934, Accuracy: 0.9169\n",
      "********************\n",
      "Epoch [3/5], Step [100/938], Loss: 0.3324, Accuracy: 0.9208\n",
      "Epoch [3/5], Step [200/938], Loss: 0.1937, Accuracy: 0.9189\n",
      "Epoch [3/5], Step [300/938], Loss: 0.3166, Accuracy: 0.9196\n",
      "Epoch [3/5], Step [400/938], Loss: 0.2646, Accuracy: 0.9191\n",
      "Epoch [3/5], Step [500/938], Loss: 0.2516, Accuracy: 0.9188\n",
      "Epoch [3/5], Step [600/938], Loss: 0.1937, Accuracy: 0.9187\n",
      "Epoch [3/5], Step [700/938], Loss: 0.2764, Accuracy: 0.9174\n",
      "Epoch [3/5], Step [800/938], Loss: 0.3597, Accuracy: 0.9171\n",
      "Epoch [3/5], Step [900/938], Loss: 0.2212, Accuracy: 0.9172\n",
      "55060 60000\n",
      "Epoch [3/5] completed in 15.57 seconds, Loss: 0.2925, Accuracy: 0.9177\n",
      "********************\n",
      "Epoch [4/5], Step [100/938], Loss: 0.6628, Accuracy: 0.9250\n",
      "Epoch [4/5], Step [200/938], Loss: 0.2062, Accuracy: 0.9226\n",
      "Epoch [4/5], Step [300/938], Loss: 0.1469, Accuracy: 0.9193\n",
      "Epoch [4/5], Step [400/938], Loss: 0.1538, Accuracy: 0.9194\n",
      "Epoch [4/5], Step [500/938], Loss: 0.4129, Accuracy: 0.9178\n",
      "Epoch [4/5], Step [600/938], Loss: 0.2981, Accuracy: 0.9179\n",
      "Epoch [4/5], Step [700/938], Loss: 0.1490, Accuracy: 0.9178\n",
      "Epoch [4/5], Step [800/938], Loss: 0.2156, Accuracy: 0.9172\n",
      "Epoch [4/5], Step [900/938], Loss: 0.2002, Accuracy: 0.9174\n",
      "55054 60000\n",
      "Epoch [4/5] completed in 16.53 seconds, Loss: 0.2885, Accuracy: 0.9176\n",
      "********************\n",
      "Epoch [5/5], Step [100/938], Loss: 0.0910, Accuracy: 0.9200\n",
      "Epoch [5/5], Step [200/938], Loss: 0.3330, Accuracy: 0.9175\n",
      "Epoch [5/5], Step [300/938], Loss: 0.2773, Accuracy: 0.9169\n",
      "Epoch [5/5], Step [400/938], Loss: 0.3208, Accuracy: 0.9157\n",
      "Epoch [5/5], Step [500/938], Loss: 0.2974, Accuracy: 0.9166\n",
      "Epoch [5/5], Step [600/938], Loss: 0.1004, Accuracy: 0.9185\n",
      "Epoch [5/5], Step [700/938], Loss: 0.2438, Accuracy: 0.9189\n",
      "Epoch [5/5], Step [800/938], Loss: 0.2925, Accuracy: 0.9196\n",
      "Epoch [5/5], Step [900/938], Loss: 0.1717, Accuracy: 0.9201\n",
      "55218 60000\n",
      "Epoch [5/5] completed in 19.04 seconds, Loss: 0.2829, Accuracy: 0.9203\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"*\" * 20)\n",
    "    start_time = time.time()\n",
    "    running_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        images, labels = data\n",
    "        images = images.view(images.size(0), -1)  # Flatten the images\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        # print(outputs, labels.data,outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        # print(total_correct, total_samples)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            accuracy = total_correct / total_samples\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], \"f\"Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    # print(f\"Outputs : {outputs}\")\n",
    "    # print(f\"Outputs Data : {outputs.data}\")\n",
    "    print(total_correct, total_samples)\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = total_correct / total_samples\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {end_time - start_time:.2f} seconds, \"f\"Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20aeeaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the model\n",
    "# model.eval()\n",
    "# eval_loss=0\n",
    "# eval_acc=0\n",
    "# with torch.no_grad():\n",
    "#     predicted=model(x_train)\n",
    "\n",
    "# # Make predictions\n",
    "# print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c4a5eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.284059, Acc: 0.920482\n",
      "Time:2.5 s\n"
     ]
    }
   ],
   "source": [
    "model.eval()  #for checking how good is your model on the test dataset  (for evaluation )\n",
    "eval_loss = 0.\n",
    "eval_acc = 0.\n",
    "start_time = time.time()\n",
    "for data in test_loader:\n",
    "    img, label = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    img=img.to(device)\n",
    "    label=label.to(device)\n",
    "    with torch.no_grad(): # because here we do not need gradients because we are just testing our model,\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "    eval_loss += loss.item()\n",
    "    _, pred = torch.max(out, 1) #getting prediction from our model for testing the result\n",
    "    eval_acc += (pred == label).float().mean()\n",
    "print(f'Test Loss: {eval_loss/len(test_loader):.6f}, Acc: {eval_acc/len(test_loader):.6f}')\n",
    "print(f'Time:{(time.time()-start_time):.1f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "608d616c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing some predictions\n",
    "images, labels = next(iter(test_loader))\n",
    "images = images.reshape(-1, 28*28).to(device)\n",
    "labels = labels.to(device)\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "#fig is a reference to the entire figure.\n",
    "#ax is an array of Axes objects, and each Axes object can be used to draw a plot. Since there are 5 subplots, ax contains 5 such objects.\n",
    "#figsize=(15, 3) sets the size of the entire figure to 15 inches wide and 3 inches tall.\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 3)) #The arguments (1, 5) specify that the figure should have 1 row of 5 columns, i.e., 5 subplots aligned horizontally.\n",
    "for i in range(5):\n",
    "    ax[i].imshow(images[i].reshape(28, 28).cpu(), cmap='gray')\n",
    "    ax[i].set_title(f'True: {labels[i].item()}, Pred: {predicted[i].item()}')\n",
    "    ax[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ebb1cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 0.1115, -0.2757, -0.0721],\n",
       "                        [-0.2917,  0.1002,  0.0712],\n",
       "                        [ 0.4092, -0.2178,  0.1892]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2099, -0.2176,  0.0430],\n",
       "                        [-0.3396,  0.0480,  0.1452],\n",
       "                        [-0.1777,  0.2699,  0.3321]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1618, -0.1161, -0.0458],\n",
       "                        [-0.0068, -0.0842, -0.1038],\n",
       "                        [-0.0530,  0.1861,  0.3739]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2809,  0.4026,  0.0467],\n",
       "                        [ 0.3103,  0.0407,  0.3026],\n",
       "                        [-0.1526, -0.1617, -0.1505]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4393, -0.3035, -0.1950],\n",
       "                        [ 0.1597, -0.1124, -0.2020],\n",
       "                        [ 0.2683,  0.1814, -0.2309]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2853, -0.3965, -0.1652],\n",
       "                        [-0.1719,  0.1789,  0.3530],\n",
       "                        [ 0.2794,  0.1661,  0.2141]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0942, -0.2127, -0.0774],\n",
       "                        [-0.0014, -0.1888, -0.2622],\n",
       "                        [-0.1646, -0.1701,  0.0870]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0409, -0.0373, -0.2770],\n",
       "                        [-0.2685,  0.1249,  0.0317],\n",
       "                        [-0.0288, -0.2947, -0.2948]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1762,  0.0849, -0.1891],\n",
       "                        [ 0.3360,  0.0776, -0.1325],\n",
       "                        [-0.1799,  0.0667, -0.3268]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2876,  0.1972,  0.1838],\n",
       "                        [ 0.1582,  0.2970, -0.1201],\n",
       "                        [-0.0513,  0.2976, -0.3464]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0521,  0.3339,  0.2315],\n",
       "                        [ 0.0956,  0.2604,  0.0641],\n",
       "                        [ 0.4104,  0.1933, -0.3649]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2960, -0.1730, -0.3445],\n",
       "                        [-0.0109, -0.1841, -0.2844],\n",
       "                        [-0.0138,  0.3687,  0.3844]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3745, -0.0506, -0.1721],\n",
       "                        [-0.1700,  0.3577, -0.2533],\n",
       "                        [ 0.1889,  0.0384,  0.2952]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0160,  0.2463,  0.2037],\n",
       "                        [-0.4692, -0.0530,  0.3716],\n",
       "                        [ 0.1002,  0.1505, -0.2098]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0412, -0.2732, -0.3105],\n",
       "                        [ 0.2112, -0.2522, -0.3536],\n",
       "                        [ 0.2373,  0.2396, -0.0983]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1556, -0.0719, -0.3178],\n",
       "                        [-0.1550, -0.2641, -0.1094],\n",
       "                        [ 0.3272,  0.3077,  0.3591]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2585, -0.3728,  0.0352],\n",
       "                        [ 0.0808, -0.0512, -0.1729],\n",
       "                        [ 0.1177,  0.4356,  0.2885]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3079,  0.3400,  0.1729],\n",
       "                        [-0.2008,  0.0381,  0.3867],\n",
       "                        [ 0.1142, -0.0722, -0.0375]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3274, -0.3099, -0.3733],\n",
       "                        [-0.3158, -0.1057, -0.2585],\n",
       "                        [ 0.1556,  0.1966,  0.4563]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3288, -0.3075,  0.0422],\n",
       "                        [ 0.0276, -0.2254,  0.0861],\n",
       "                        [-0.0858, -0.3512, -0.2175]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.4203,  0.2603,  0.3531],\n",
       "                        [-0.1046,  0.3501, -0.0871],\n",
       "                        [ 0.0105,  0.2457,  0.2936]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4374,  0.2609, -0.2385],\n",
       "                        [ 0.3515, -0.2822, -0.2517],\n",
       "                        [-0.2875, -0.3750, -0.0835]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4393,  0.2466,  0.3204],\n",
       "                        [-0.2899, -0.2832,  0.0031],\n",
       "                        [-0.0349, -0.3165, -0.2654]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3325,  0.4263,  0.1388],\n",
       "                        [-0.0665, -0.2036, -0.1755],\n",
       "                        [-0.3238,  0.0708, -0.2448]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2960,  0.3606, -0.2127],\n",
       "                        [ 0.2467,  0.3314, -0.2745],\n",
       "                        [ 0.1210,  0.4136,  0.1651]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2423,  0.4284, -0.1142],\n",
       "                        [ 0.0044, -0.0855, -0.2422],\n",
       "                        [-0.2051, -0.1304, -0.0181]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0008,  0.1777, -0.2778],\n",
       "                        [ 0.2001, -0.1589, -0.2816],\n",
       "                        [-0.2032, -0.2988, -0.3529]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1030,  0.2423, -0.1943],\n",
       "                        [ 0.3792,  0.0006, -0.3677],\n",
       "                        [ 0.3531, -0.1281, -0.2128]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1483, -0.1767, -0.3210],\n",
       "                        [ 0.1894, -0.1173, -0.1511],\n",
       "                        [-0.2760, -0.2909, -0.1653]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2602,  0.4232,  0.3488],\n",
       "                        [ 0.0494,  0.0213,  0.0764],\n",
       "                        [-0.2735, -0.2231, -0.2263]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1012, -0.1574, -0.1986],\n",
       "                        [ 0.0803, -0.2363, -0.2231],\n",
       "                        [-0.2937, -0.1065, -0.1342]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1646,  0.0565, -0.1231],\n",
       "                        [-0.1634, -0.0175, -0.3231],\n",
       "                        [ 0.3346, -0.1697,  0.3157]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([ 0.1543, -0.0072,  0.1853,  0.1987, -0.0600,  0.0624, -0.0597,  0.1592,\n",
       "                       0.0666,  0.3398,  0.3921, -0.2322,  0.3275,  0.1916,  0.0174,  0.0839,\n",
       "                       0.1003,  0.2822, -0.2385, -0.2970, -0.1190,  0.1148, -0.0207, -0.0606,\n",
       "                       0.1142,  0.0876, -0.2948,  0.1468,  0.1203,  0.0824,  0.0279,  0.0336])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[ 4.8214e-02, -3.5904e-02,  4.4400e-02],\n",
       "                        [ 3.3351e-02, -3.8531e-02, -2.4894e-02],\n",
       "                        [-1.5275e-02,  4.5966e-02,  3.7307e-02]],\n",
       "              \n",
       "                       [[-7.1987e-02,  5.6674e-02, -5.9142e-02],\n",
       "                        [-7.0219e-03,  7.0505e-02, -9.7134e-02],\n",
       "                        [-2.2607e-02,  2.0190e-02,  5.5022e-03]],\n",
       "              \n",
       "                       [[-4.2655e-02,  7.5226e-02, -3.3174e-02],\n",
       "                        [ 1.7680e-02, -2.8148e-02,  8.9520e-02],\n",
       "                        [ 2.4569e-02,  2.5265e-02,  9.9498e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.7221e-02,  5.6556e-02, -1.7184e-02],\n",
       "                        [-5.0842e-03,  6.0089e-02, -2.5913e-02],\n",
       "                        [-8.0830e-02,  9.8567e-02,  5.7854e-02]],\n",
       "              \n",
       "                       [[-1.8385e-02, -8.7657e-02, -3.5145e-02],\n",
       "                        [ 6.7348e-02, -6.4781e-02, -6.0865e-02],\n",
       "                        [ 7.0553e-02,  4.1977e-03, -2.8946e-02]],\n",
       "              \n",
       "                       [[-6.7859e-02,  1.7021e-02,  4.1438e-02],\n",
       "                        [-9.5243e-02, -4.2395e-02,  1.1230e-01],\n",
       "                        [ 7.6252e-04,  3.1131e-02,  2.6830e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.3833e-02,  3.1615e-02, -3.6965e-02],\n",
       "                        [-3.2587e-02, -5.2056e-03,  1.3076e-02],\n",
       "                        [ 3.9470e-02,  3.3222e-02,  2.2323e-02]],\n",
       "              \n",
       "                       [[ 9.4025e-03, -1.9239e-03, -1.6879e-03],\n",
       "                        [-2.9196e-03,  1.7408e-02, -8.0362e-03],\n",
       "                        [ 7.0620e-02,  6.9084e-02,  3.7852e-02]],\n",
       "              \n",
       "                       [[ 1.7833e-02, -6.7540e-02, -5.7270e-03],\n",
       "                        [-3.3702e-02,  5.2198e-02,  2.3433e-02],\n",
       "                        [ 8.7002e-02,  1.0426e-01,  4.3958e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3585e-02, -5.2942e-02, -3.0822e-02],\n",
       "                        [-1.1285e-02, -1.7866e-02, -6.6826e-02],\n",
       "                        [-1.8237e-02, -2.2392e-02,  1.8620e-02]],\n",
       "              \n",
       "                       [[ 3.0942e-02, -2.5893e-03,  4.2917e-02],\n",
       "                        [-3.7059e-02, -5.5121e-02, -2.1623e-02],\n",
       "                        [ 3.3401e-02, -4.0567e-02,  6.7871e-03]],\n",
       "              \n",
       "                       [[-2.5423e-02,  2.4759e-02, -4.2269e-02],\n",
       "                        [-9.3466e-03,  2.0943e-02, -1.0060e-02],\n",
       "                        [ 4.1879e-03, -2.2721e-02,  2.2521e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.9308e-02, -6.6009e-02, -1.4800e-02],\n",
       "                        [-1.2618e-02, -2.0572e-02,  1.4755e-02],\n",
       "                        [-4.5917e-02, -1.4111e-02, -4.2413e-02]],\n",
       "              \n",
       "                       [[ 1.1708e-01,  2.7592e-02, -7.3119e-02],\n",
       "                        [ 2.2282e-02, -3.4813e-02,  4.1156e-02],\n",
       "                        [-2.6100e-02, -7.8771e-02, -1.3128e-01]],\n",
       "              \n",
       "                       [[-2.8758e-02,  6.2507e-02, -3.0437e-03],\n",
       "                        [-7.4319e-02,  3.9175e-02,  1.6116e-01],\n",
       "                        [-1.3212e-02, -7.2714e-02, -6.0234e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.6819e-02, -1.1390e-02, -4.5816e-02],\n",
       "                        [ 9.0730e-03,  8.9274e-02,  1.0894e-02],\n",
       "                        [ 3.6565e-02,  6.3463e-02,  4.4784e-02]],\n",
       "              \n",
       "                       [[ 1.1426e-02, -9.3829e-03,  1.0411e-02],\n",
       "                        [ 5.1155e-03, -6.8708e-02, -8.9306e-02],\n",
       "                        [ 1.2104e-02,  6.2074e-03,  2.1774e-02]],\n",
       "              \n",
       "                       [[-1.2674e-01,  7.4494e-02,  1.1044e-01],\n",
       "                        [-9.7944e-02, -4.2318e-02,  3.0026e-02],\n",
       "                        [-1.3523e-01, -3.9436e-02, -1.9296e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 4.4193e-02, -5.8177e-03,  1.6318e-02],\n",
       "                        [-3.5356e-02, -3.4401e-03,  4.5697e-02],\n",
       "                        [ 2.6615e-02,  8.1725e-02,  3.8724e-02]],\n",
       "              \n",
       "                       [[-1.6029e-02, -1.4202e-02,  4.2538e-02],\n",
       "                        [ 6.1462e-02, -1.9921e-02,  4.3930e-02],\n",
       "                        [ 7.7593e-02, -1.4080e-02, -9.8552e-02]],\n",
       "              \n",
       "                       [[ 5.7686e-02, -3.6554e-03, -2.1807e-02],\n",
       "                        [ 3.0660e-02,  5.5744e-02,  2.8760e-02],\n",
       "                        [ 7.1315e-02, -4.8190e-02,  5.1703e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 6.2843e-02,  5.9304e-02,  1.9859e-02],\n",
       "                        [ 8.5304e-02,  4.6500e-02,  1.8158e-02],\n",
       "                        [ 2.6419e-02,  4.6634e-02,  1.0884e-01]],\n",
       "              \n",
       "                       [[-5.7642e-02, -5.5430e-02,  2.8423e-03],\n",
       "                        [ 3.5505e-02, -2.6546e-02, -6.7129e-02],\n",
       "                        [-8.3259e-02, -8.6554e-03, -2.2416e-03]],\n",
       "              \n",
       "                       [[-7.0845e-02,  4.9803e-02, -8.9852e-03],\n",
       "                        [ 4.5225e-02,  1.7778e-02,  1.0355e-02],\n",
       "                        [ 7.7227e-02, -2.2996e-02, -3.7579e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.6958e-02,  4.7183e-02,  1.1219e-01],\n",
       "                        [-3.1266e-02, -2.5275e-02,  6.2906e-02],\n",
       "                        [ 2.3343e-02, -4.8165e-02,  5.5190e-03]],\n",
       "              \n",
       "                       [[-5.9025e-02, -6.3002e-02,  1.0858e-02],\n",
       "                        [-7.3402e-02, -2.7649e-02, -8.1272e-02],\n",
       "                        [-8.9385e-02, -8.3214e-02, -2.2205e-02]],\n",
       "              \n",
       "                       [[ 6.0383e-02,  3.9530e-02,  7.0150e-02],\n",
       "                        [-6.2411e-02,  7.2638e-02,  1.1462e-01],\n",
       "                        [-8.7900e-02, -9.0885e-02, -6.3987e-05]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.6965e-02,  3.6770e-02, -1.6680e-02],\n",
       "                        [ 1.0008e-01,  9.9182e-02,  8.2569e-02],\n",
       "                        [ 2.7983e-02,  1.1282e-01,  6.8459e-02]],\n",
       "              \n",
       "                       [[-1.7328e-02, -3.6153e-02,  1.3080e-03],\n",
       "                        [ 1.8478e-02,  7.2612e-03, -8.4396e-02],\n",
       "                        [ 3.6509e-02, -1.7794e-02,  5.9440e-03]],\n",
       "              \n",
       "                       [[ 3.5736e-02,  2.2521e-02,  1.0884e-01],\n",
       "                        [-7.4980e-02,  4.8921e-02,  4.6082e-02],\n",
       "                        [ 3.2740e-02, -5.1507e-02, -4.9683e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.7604e-03,  4.2245e-02,  7.0308e-02],\n",
       "                        [-3.8824e-02, -3.7375e-02,  2.4464e-03],\n",
       "                        [ 2.9322e-02,  1.1368e-02,  2.4533e-02]],\n",
       "              \n",
       "                       [[-3.5405e-02, -8.3472e-03, -2.1831e-02],\n",
       "                        [ 3.8799e-02,  5.9254e-02, -2.7087e-02],\n",
       "                        [-3.7382e-02,  6.8611e-03,  1.0889e-02]],\n",
       "              \n",
       "                       [[ 3.1565e-02, -2.1962e-02,  6.1072e-03],\n",
       "                        [ 6.6405e-03,  8.5119e-02,  3.7565e-02],\n",
       "                        [ 9.3382e-02,  6.2706e-02, -7.6000e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-8.9441e-02, -2.4387e-02, -2.5899e-02],\n",
       "                        [ 3.5629e-02, -7.5629e-02, -2.1594e-03],\n",
       "                        [-3.0695e-02,  4.3019e-04,  5.3370e-02]],\n",
       "              \n",
       "                       [[ 6.6426e-03, -3.4146e-02,  6.1739e-02],\n",
       "                        [-1.0127e-02,  7.7371e-03, -7.5303e-02],\n",
       "                        [ 2.0790e-02,  4.5110e-02,  2.3189e-02]],\n",
       "              \n",
       "                       [[ 1.9512e-02,  6.9665e-02,  3.4081e-03],\n",
       "                        [-2.9375e-02, -3.2330e-03, -4.7948e-02],\n",
       "                        [ 4.2139e-02, -9.3716e-03,  6.5352e-02]]]])),\n",
       "             ('conv2.bias',\n",
       "              tensor([-0.0164,  0.0400,  0.0109, -0.0227, -0.0008, -0.0518, -0.0156, -0.0443,\n",
       "                      -0.0635, -0.0408,  0.0475,  0.0052, -0.0513, -0.0249, -0.0557, -0.0277,\n",
       "                       0.0342, -0.0348,  0.0323,  0.0216, -0.0703, -0.0043, -0.0387,  0.0220,\n",
       "                      -0.0371, -0.0394, -0.0086,  0.0066,  0.0352, -0.0220, -0.0086,  0.0408,\n",
       "                      -0.0002,  0.0197, -0.0500,  0.0360, -0.0133, -0.0188, -0.0562,  0.0063,\n",
       "                       0.0322,  0.0454, -0.0487,  0.0383,  0.0339, -0.0122, -0.0246, -0.0178,\n",
       "                       0.0044,  0.0230, -0.0020, -0.0216, -0.0300, -0.0344, -0.0487, -0.0660,\n",
       "                       0.0102,  0.0222, -0.0008, -0.0164, -0.0582, -0.0005, -0.0279,  0.0455])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0075,  0.0266,  0.0320,  ...,  0.0154, -0.0060,  0.0008],\n",
       "                      [ 0.0221,  0.0058,  0.0022,  ..., -0.0031, -0.0026,  0.0101],\n",
       "                      [-0.0051,  0.0080,  0.0057,  ...,  0.0180, -0.0105,  0.0073],\n",
       "                      ...,\n",
       "                      [ 0.0113, -0.0223, -0.0010,  ...,  0.0004,  0.0116, -0.0060],\n",
       "                      [-0.0052, -0.0157,  0.0225,  ...,  0.0034, -0.0003, -0.0046],\n",
       "                      [-0.0077,  0.0214,  0.0492,  ...,  0.0292, -0.0035,  0.0108]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0045, -0.0027,  0.0010, -0.0165,  0.0222, -0.0032, -0.0013, -0.0123,\n",
       "                       0.0075, -0.0029,  0.0080, -0.0011, -0.0007, -0.0141, -0.0058, -0.0059,\n",
       "                       0.0055,  0.0201,  0.0003, -0.0192,  0.0094,  0.0034, -0.0050,  0.0026,\n",
       "                      -0.0061, -0.0011,  0.0136, -0.0012, -0.0053,  0.0092,  0.0230,  0.0056,\n",
       "                       0.0050, -0.0347, -0.0146, -0.0218,  0.0004,  0.0196, -0.0064, -0.0137,\n",
       "                       0.0055,  0.0102, -0.0133,  0.0246,  0.0244,  0.0048, -0.0016,  0.0095,\n",
       "                      -0.0137, -0.0072,  0.0005, -0.0224, -0.0048, -0.0063,  0.0025,  0.0053,\n",
       "                      -0.0122, -0.0234,  0.0180,  0.0102,  0.0194, -0.0208,  0.0173, -0.0119,\n",
       "                       0.0168,  0.0327, -0.0129,  0.0099, -0.0143, -0.0106, -0.0231,  0.0009,\n",
       "                       0.0012,  0.0089, -0.0109, -0.0082,  0.0181, -0.0061, -0.0041, -0.0162,\n",
       "                       0.0058,  0.0185, -0.0090,  0.0189,  0.0080, -0.0030,  0.0207, -0.0160,\n",
       "                       0.0047, -0.0003, -0.0125,  0.0014, -0.0018,  0.0007,  0.0033,  0.0199,\n",
       "                      -0.0121, -0.0150,  0.0075,  0.0103,  0.0028,  0.0174,  0.0005,  0.0139,\n",
       "                       0.0031,  0.0044, -0.0096,  0.0165, -0.0103,  0.0255, -0.0132, -0.0183,\n",
       "                       0.0076,  0.0188, -0.0090,  0.0149,  0.0243,  0.0003,  0.0007, -0.0166,\n",
       "                       0.0030, -0.0105, -0.0202,  0.0047,  0.0035,  0.0139, -0.0104,  0.0187])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.0921, -0.1168,  0.0775,  ...,  0.0112,  0.0289,  0.0584],\n",
       "                      [-0.1248,  0.0180, -0.0883,  ..., -0.0627, -0.1003, -0.0105],\n",
       "                      [-0.0290, -0.0800,  0.0392,  ...,  0.0068,  0.0913,  0.0531],\n",
       "                      ...,\n",
       "                      [-0.0702, -0.0955, -0.0620,  ..., -0.0250,  0.0664, -0.0245],\n",
       "                      [-0.0317,  0.0872,  0.0544,  ..., -0.0191,  0.0140, -0.0647],\n",
       "                      [-0.0328, -0.0019,  0.0123,  ..., -0.0224,  0.0243,  0.0751]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0026, -0.0718, -0.0086, -0.0763, -0.0512, -0.0462, -0.0058, -0.0348,\n",
       "                      -0.0499, -0.0611]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdcbfead",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'./LogisticRegression.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f4707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9888adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b7c489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d77447bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m num_epochs = \u001b[32m5\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m      3\u001b[39m model.train()\n\u001b[32m      4\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepLearning/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepLearning/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepLearning/env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepLearning/env/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepLearning/env/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepLearning/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepLearning/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1756\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1756\u001b[39m     forward_call = (\u001b[38;5;28mself\u001b[39m._slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward)\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model, criterion, optimizer, and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'mnist_cnn.pth')\n",
    "# Load the model\n",
    "model = SimpleCNN().to(device)\n",
    "model.load_state_dict(torch.load('mnist_cnn.pth', map_location=device))\n",
    "# Evaluate the loaded model\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "print(f'Loaded Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(model, test_loader, device):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(15, 3))\n",
    "    for i in range(10):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "        ax.set_title(f'Pred: {predicted[i].item()}\\nTrue: {labels[i].item()}')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "visualize_predictions(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9231c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in safetensors format\n",
    "import safetensors.torch\n",
    "safetensors.save_model(model, 'mnist_cnn.safetensors')\n",
    "# Load the model from safetensors format\n",
    "model = safetensors.load_model(SimpleCNN(), 'mnist_cnn.safetensors', map_location=device)\n",
    "# Evaluate the loaded model from safetensors\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "print(f'Loaded Safetensors Model - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "# Clean up      \n",
    "import os  \n",
    "os.remove('mnist_cnn.pth')\n",
    "os.remove('mnist_cnn.safetensors')\n",
    "print(\"Cleaned up model files.\")\n",
    "# End of the script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Script executed successfully.\")\n",
    "    sys.exit(0)\n",
    "# End of the script\n",
    "# End of the script "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
